{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from spacy.cli.train import train\n",
    "from spacy.cli.evaluate import evaluate\n",
    "from spacy.tokens import DocBin\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/bio/fr.bio/fr.sentences.bio\"\n",
    "tag_mapping = {\n",
    "    \"O\": 0,\n",
    "    \"B-LOC-DEP\": 1,\n",
    "    \"B-LOC-ARR\": 2,\n",
    "    \"I-LOC-DEP\": 3,\n",
    "    \"I-LOC-ARR\": 4\n",
    "}\n",
    "\n",
    "with open(data_file) as f:\n",
    "    data = f.read()\n",
    "    \n",
    "tokens = []\n",
    "ner_tags = []\n",
    "spans = []\n",
    "text = []\n",
    "sentences = re.split(r'(?<=[.!?] O)\\n', data)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = []\n",
    "    tags = []\n",
    "    this_sentence_spans = []\n",
    "    word_tag_pairs = sentence.split(\"\\n\")\n",
    "    \n",
    "    for pair in word_tag_pairs:\n",
    "        if pair.split():\n",
    "            (word, tag) = pair.split(\" \")\n",
    "            if tag != \"O\":\n",
    "                start_offset = len(\" \".join(words)) + (1 if words else 0)\n",
    "                end_offset = start_offset + len(word)\n",
    "                this_sentence_spans.append((start_offset, end_offset, tag))\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "        \n",
    "    sentence_text = \" \".join(words)\n",
    "    text.append(sentence_text)\n",
    "    \n",
    "    tokens.append(words)    \n",
    "    ner_tags.append(tags)\n",
    "    spans.append(this_sentence_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for (index, sentence_text) in enumerate(text):\n",
    "    doc = nlp(sentence_text)\n",
    "    ents = []\n",
    "    for start, end, label in spans[index]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        ents.append(span)\n",
    "        \n",
    "    doc.ents = ents        \n",
    "    data.append(doc)\n",
    "    \n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = DocBin()\n",
    "test_db = DocBin()\n",
    "\n",
    "for doc in train_data:\n",
    "    train_db.add(doc)\n",
    "for doc in test_data:\n",
    "    test_db.add(doc)\n",
    "    \n",
    "train_db.to_disk(\"tor_ner_train.spacy\")                \n",
    "test_db.to_disk(\"tor_ner_test.spacy\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  -----------  -----------  --------  -------  -------  -------  -------  ------  ------  ------  ------\n",
      "  0       0          0.00        78.14       269.21     56.22    59.13    12.69    10.28     0.49    0.00    0.00    0.00    0.23\n",
      "  0     200        491.26      4051.27      7550.68   2599.34    91.04    88.04    82.20    90.48   90.41   90.47   90.35    0.89\n",
      "  0     400        547.35      3478.36      5221.82    635.04    92.57    87.33    82.54    94.13   86.93   87.12   86.74    0.88\n",
      "  0     600        860.68      3656.24      5376.61    993.76    93.57    88.33    84.41    89.96   94.29   94.26   94.32    0.91\n",
      "  0     800       1630.17      3959.75      6155.76   1428.99    94.10    89.64    85.41    94.20   94.74   94.86   94.62    0.92\n",
      "  1    1000       2110.25      4122.01      7107.16   1233.78    94.47    87.84    84.34    88.29   92.52   92.81   92.23    0.91\n",
      "  1    1200       1702.33      4492.46      7944.56    667.44    94.91    89.55    86.27    94.66   94.96   94.86   95.06    0.93\n",
      "  2    1400       2078.91      4653.16      8831.33    815.25    95.29    89.78    86.52    92.12   94.41   94.44   94.39    0.93\n",
      "  2    1600       5286.68      5462.56     10340.85   2877.94    95.43    89.73    86.49    93.61   94.71   94.78   94.64    0.93\n",
      "  3    1800       3851.75      5615.96     11211.48   1306.48    95.64    90.06    86.56    94.23   94.06   93.97   94.16    0.93\n",
      "  4    2000       4819.69      6567.38     12147.84   1152.23    95.66    90.48    87.49    93.54   95.18   94.97   95.40    0.93\n",
      "  5    2200       6155.78      6932.66     12605.46   1586.76    95.83    89.94    87.05    93.15   94.67   94.46   94.87    0.93\n",
      "  7    2400       6981.09      7152.54     12539.10   1502.70    95.87    89.66    86.89    94.18   94.30   94.34   94.25    0.93\n",
      "  8    2600       7544.36      6370.72     11099.51   1486.25    95.92    89.83    87.14    92.13   94.22   94.01   94.44    0.93\n",
      " 10    2800       6995.51      5370.99      9646.21   1322.92    95.91    89.24    86.46    93.01   94.24   94.07   94.42    0.93\n",
      " 11    3000       8035.94      4637.76      8738.70   1943.11    95.93    89.76    86.93    92.83   94.68   94.73   94.62    0.93\n",
      " 13    3200       7618.18      3850.00      8383.82   1511.52    95.85    89.86    87.06    90.64   94.26   94.15   94.37    0.93\n",
      " 14    3400       7532.91      3462.47      7267.98   1500.95    95.96    90.19    87.40    92.83   95.28   95.22   95.33    0.93\n",
      " 16    3600       8164.78      2998.56      6773.97   1572.54    95.87    89.59    86.77    93.99   95.39   95.28   95.50    0.93\n",
      " 17    3800       6968.64      2816.44      6223.81   1166.74    95.95    89.53    86.74    92.13   95.03   94.85   95.22    0.93\n",
      " 19    4000       6997.26      2469.63      5810.32   1118.78    95.85    89.70    87.02    93.59   95.09   95.00   95.17    0.93\n",
      " 20    4200       8949.63      2231.00      5713.56   1696.03    95.90    89.53    86.87    93.90   95.00   94.99   95.01    0.93\n",
      " 22    4400       7491.64      2031.87      5708.05   1194.32    95.89    89.81    86.96    92.68   95.03   95.08   94.99    0.93\n",
      " 23    4600       7609.32      1869.41      5045.68   1452.08    95.84    88.51    85.79    92.21   94.83   94.85   94.81    0.93\n",
      " 25    4800       7989.85      1743.26      4822.59   1430.76    95.91    90.10    87.23    94.61   95.27   95.18   95.36    0.93\n",
      " 26    5000       7657.01      1503.55      4586.48   1199.53    95.94    89.41    86.62    92.97   95.07   95.00   95.15    0.93\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "spacy/model-last\n"
     ]
    }
   ],
   "source": [
    "train(\"spacy_config_ner.cfg\", output_path=\"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9850639263950293,\n",
       " 'token_p': 0.7914027960329788,\n",
       " 'token_r': 0.9159838191059019,\n",
       " 'token_f': 0.8491482235292231,\n",
       " 'tag_acc': 0.9593681439227731,\n",
       " 'sents_p': 0.9381294964028777,\n",
       " 'sents_r': 0.9213377296278851,\n",
       " 'sents_f': 0.9296577946768061,\n",
       " 'dep_uas': 0.8940712205205299,\n",
       " 'dep_las': 0.8661730246508577,\n",
       " 'dep_las_per_type': {'nsubj': {'p': 0.9899598393574297,\n",
       "   'r': 0.8257956448911222,\n",
       "   'f': 0.9004566210045661},\n",
       "  'root': {'p': 0.9549597855227882,\n",
       "   'r': 0.8389072067828545,\n",
       "   'f': 0.8931795386158475},\n",
       "  'xcomp': {'p': 0.9415730337078652,\n",
       "   'r': 0.8275181040157998,\n",
       "   'f': 0.8808689558514367},\n",
       "  'det': {'p': 0.9935248887090247,\n",
       "   'r': 0.9250188394875659,\n",
       "   'f': 0.9580487804878048},\n",
       "  'obj': {'p': 0.9031890660592256,\n",
       "   'r': 0.8786703601108034,\n",
       "   'f': 0.8907610221847796},\n",
       "  'case': {'p': 0.9796151647932939,\n",
       "   'r': 0.9180503481521157,\n",
       "   'f': 0.9478341013824885},\n",
       "  'nmod': {'p': 0.8415692191625802,\n",
       "   'r': 0.7925399644760213,\n",
       "   'f': 0.8163190633004025},\n",
       "  'obl:arg': {'p': 0.7148014440433214,\n",
       "   'r': 0.8336842105263158,\n",
       "   'f': 0.7696793002915453},\n",
       "  'obl:mod': {'p': 0.6243705941591138,\n",
       "   'r': 0.6001936108422071,\n",
       "   'f': 0.6120434353405726},\n",
       "  'amod': {'p': 0.9279475982532751,\n",
       "   'r': 0.9110396570203644,\n",
       "   'f': 0.9194159004867496},\n",
       "  'mark': {'p': 0.9882032667876588,\n",
       "   'r': 0.9444926279271466,\n",
       "   'f': 0.9658536585365853},\n",
       "  'acl': {'p': 0.59375, 'r': 0.5277777777777778, 'f': 0.5588235294117648},\n",
       "  'advcl': {'p': 0.8882488479262672,\n",
       "   'r': 0.8604910714285714,\n",
       "   'f': 0.8741496598639455},\n",
       "  'obl:agent': {'p': 0.4117647058823529, 'r': 0.875, 'f': 0.56},\n",
       "  'iobj': {'p': 0.9747474747474747,\n",
       "   'r': 0.9146919431279621,\n",
       "   'f': 0.943765281173594},\n",
       "  'expl:comp': {'p': 1.0, 'r': 0.9948320413436692, 'f': 0.9974093264248705},\n",
       "  'advmod': {'p': 0.9664429530201343,\n",
       "   'r': 0.9494505494505494,\n",
       "   'f': 0.9578713968957872},\n",
       "  'aux:tense': {'p': 0.9146341463414634,\n",
       "   'r': 0.9868421052631579,\n",
       "   'f': 0.949367088607595},\n",
       "  'cop': {'p': 1.0, 'r': 0.9876543209876543, 'f': 0.9937888198757764},\n",
       "  'dep': {'p': 0.8978102189781022,\n",
       "   'r': 0.8891566265060241,\n",
       "   'f': 0.8934624697336562},\n",
       "  'cc': {'p': 1.0, 'r': 0.9416666666666667, 'f': 0.9699570815450643},\n",
       "  'expl:subj': {'p': 0.7027027027027027, 'r': 1.0, 'f': 0.8253968253968255},\n",
       "  'fixed': {'p': 0.3333333333333333,\n",
       "   'r': 0.07142857142857142,\n",
       "   'f': 0.11764705882352941},\n",
       "  'appos': {'p': 0.4444444444444444,\n",
       "   'r': 0.3333333333333333,\n",
       "   'f': 0.380952380952381},\n",
       "  'acl:relcl': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'flat:name': {'p': 0.7333333333333333, 'r': 0.44, 'f': 0.5499999999999999},\n",
       "  'conj': {'p': 0.7142857142857143,\n",
       "   'r': 0.5825242718446602,\n",
       "   'f': 0.6417112299465241},\n",
       "  'parataxis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'nummod': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'ccomp': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'aux:pass': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'ents_p': 0.9499770536943553,\n",
       " 'ents_r': 0.9515054010572283,\n",
       " 'ents_f': 0.9507406131588012,\n",
       " 'ents_per_type': {'B-LOC-DEP': {'p': 0.9573200992555831,\n",
       "   'r': 0.953534354918438,\n",
       "   'f': 0.9554234769687964},\n",
       "  'B-LOC-ARR': {'p': 0.9483267716535433,\n",
       "   'r': 0.955379276152702,\n",
       "   'f': 0.9518399604840702},\n",
       "  'I-LOC-ARR': {'p': 0.9448275862068966,\n",
       "   'r': 0.8838709677419355,\n",
       "   'f': 0.9133333333333333},\n",
       "  'I-LOC-DEP': {'p': 0.8855421686746988,\n",
       "   'r': 0.9423076923076923,\n",
       "   'f': 0.9130434782608695}},\n",
       " 'speed': 4403.774818956297}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"spacy/model-last\", \"tor_ner_test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montpellier B-LOC-DEP 4 5\n",
      "Paris B-LOC-ARR 6 7\n"
     ]
    }
   ],
   "source": [
    "ft_nlp = spacy.load(\"spacy/model-last\")\n",
    "\n",
    "doc = ft_nlp(\"Je veux aller de Montpellier à Paris\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent.start, ent.end)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-nlp-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
